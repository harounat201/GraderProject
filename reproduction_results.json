{
  "report": {
    "score": {
      "earned": 90,
      "max": 100,
      "letter": "A-"
    },
    "summary": "The student provides a comprehensive overview of their automated grading tool, clearly outlining the problem, technical design, and evaluation. They thoughtfully incorporate design principles and reflect critically on the tool's performance based on TA feedback. Minor deductions were applied for opportunities to further strengthen the integration of design tenets and for potential organizational improvements.",
    "criteria": [
      {
        "name": "Problem Definition & Exigence",
        "earned": 20,
        "max": 20,
        "feedback": "The student clearly articulates the 'why' behind the project, effectively connecting budget constraints and enrollment size to the need for a CS-driven solution. They successfully link CS principles to specific administrative needs within the DECISION 616 course."
      },
      {
        "name": "Technical Design & Methodology",
        "earned": 23,
        "max": 25,
        "feedback": "The student thoroughly explains the 'Main vs. Task' abstraction and demonstrates a strong understanding of the .xlsx XML structure and Openpyxl logic. Minor deduction for not providing even more in-depth detail."
      },
      {
        "name": "Design Principles (The 5 Tenets)",
        "earned": 13,
        "max": 15,
        "feedback": "The student mentions the five tenets (Integrity, Efficiency, Simplicity, Transparency, and Reproducibility). While the tenets are present throughout the narrative, there are opportunities to integrate them more consistently into the description of specific product features."
      },
      {
        "name": "Critical Reflection & Evaluation",
        "earned": 19,
        "max": 20,
        "feedback": "The student candidly discusses technical debt (loss of code), 'fence-post' errors, and analyzes TA feedback to suggest specific improvements for Version 2 of the autograder."
      },
      {
        "name": "Organization & Professionalism",
        "earned": 8,
        "max": 10,
        "feedback": "The report has a logical flow with clear headings and uses 'Figures' appropriately. The tone is professional and suitable for an Independent Study report. Some transitions could be smoother."
      },
      {
        "name": "Format & Documentation",
        "earned": 7,
        "max": 10,
        "feedback": "The report includes project stakeholders and metadata but lacks consistent labeling of figures/sections. Needs some formatting adjustments."
      }
    ],
    "incorrect_cells": [],
    "mode": "ai_unified",
    "rubric_source": "provided_excel_raw"
  },
  "prompt": "\n    You are an expert AI Grader.\n    \n    \n        You are provided with a RUBRIC.\n        1. Evaluate the Student's work based SOLELY on the descriptions in the Rubric.\n        2. You must strictly follow the Rubric's point allocations.\n        \n    \n    TARGET SCHEMA (JSON ONLY):\n    \n    {\n      \"score\": {\n        \"earned\": number,\n        \"max\": number,\n        \"letter\": string\n      },\n      \"summary\": string,\n      \"criteria\": [ \n        { \n          \"name\": string, \n          \"earned\": number, \n          \"max\": number, \n          \"feedback\": string \n        }\n      ],\n      \"incorrect_cells\": [\n        { \n          \"sheet\": string, \n          \"cell\": string, \n          \"expected\": string, \n          \"actual\": string, \n          \"explanation\": string \n        }\n      ]\n    }\n    \n    \n    INSTRUCTIONS FOR SCHEMA:\n    1. \"score\": Always populate this.\n    2. \"criteria\": Populate this if a Rubric is provided OR if you are grading an Essay/Text assignment. List specific grading criteria or tasks.\n    3. \"incorrect_cells\": Populate this ONLY if you are comparing Excel Data against an Answer Key and find specific cell discrepancies.\n    \n    STUDENT SUBMISSION TEXT:\n    Duke University\n\n\n\n\n\n\n\nGrade Automation for Graduate Coursework: \nA Product Overview\n\n\n\n\n\n\n\n\nHarouna Thiam\nCOMPSCI 391: Independent Study\nProf. Robert Duvall\nDr. Salman Azhar\n10 Dec. 2025\nThis write-up covers the introduction, overview, development, and evaluation of a course assignment auto grader for the purposes of assisting in the evaluation of marks for select assignments of DECISION 616 course Business Computer Applications in the Duke Fuqua School of Business, taught by Dr. Salman Azhar. I think it is impossible to entirely separate discussion of the software tool from discussion of the course and grading protocols, so the entirety of the software tool will be discussed, as well as necessary operational information (course workflows, etc.) from DECISION 616 when appropriate.\n\nEXIGENCE\n\n\tThe primary context for the development of such a product comes all the way back to budget cuts and headcount. Unlike other semesters and courses, particular courses have trouble garnering headcount for instructional Teachers Assistants, often due to issues such as funding cuts. Additionally, particular course structures relay a need for more of a Grading Assistant focus, where assistants primarily grade courses rather than teach instructional content. With enrollment numbers for particular courses at Fuqua exceeding 400+ students, Grading Assistants and particularly administrators have a hard time delegating and optimizing for grading completion. Such a predicament contextualizes a search for some kind of optimization in the grading process. Computer science principles are sought out for such challenges, and in particular in the administrative and grading space, algorithmic innovation can be utilized. To build an \u2018algorithm\u2019, one should be able to understand and piece together a reproducible and scalable set of instructions. In the scope of computer science and software, such a set of instructions should be programmatic and operable by a computer or set of computers. From a course administrative point of view, one would find the employment of a computer algorithm particularly useful, as it may be able to assist in a number of automated tasks. These relate, more or less, to: the assignment of grades, delegation of TA (Teaching Assistant) tasks, submission filtering, etc. And in the context of very large, primarily assignment oriented, grading focused courses (like DECISION 616), a streamlined and algorithmic approach would handle many such administrative and grading-based grievances, such as naming-convention enforcement, grade penalty enforcement (computing lateness), plagiarism detection, rubric enforcement and evaluation, and much, much more. Such communication brings us to the implementation of a plethora of grading tools, including Gradescope, CodeRunner, DataCamp, Canvas, and others. Each has made waves in solving a particular administrative issue amid automating grading. Gradescope, for example, has built-in test-case evaluating software for grading code samples. Other platforms, like Canvas, a popular LMS (Learning Management Service) used at hundreds of institutions including Duke, have native quiz and assignment functionality which is beneficial for many course and content structures. \n\nSPECIFICITY, PROBLEM SPACE\n\nDuke Fuqua\u2019s DECISION 616 course is characterized by many of the factors previously described. Hundreds of students completing the course concurrently, primarily assignment-focused course structure, and relatively low Teaching Assistant count for the student size. Various grading software and LMS\u2019 offer tools to assist with such a course structure\u2014not limited to, automated multiple-choice quiz grading, assignment portals for submission, and code evaluation software (supporting multiple languages). However, DECISION 616 offers a unique challenge. The assignment format is not that of a multiple-choice quiz or code. Rather, each of 400+ students submit Excel assignments. Excel is a desktop software product from the Microsoft Office suite, which offers a spreadsheet workbook with cells. Each cell may contain some arbitrary value, as well as a specialized sequence of characters called a \u2018formula\u2019, which may alter the given cell or a range of cells in a multitude of ways. Excel spreadsheets themselves are organized into workbooks. Under the hood, Excel spreadsheets aren\u2019t simply a document, but a .zip file which contains packets of XML code. Essentially, it is a package of interlinked XML files that together determine the structure of the assignment. It is important to note the structure as our tech stack surrounds the utilization of tools that parse these files to find essential information. \n\nIn general, none of the regularly available platforms or LMSs are suitable for the widespread Excel auto grading of a large class. There do exist external programs that are able to evaluate Excel files. However, these options (note: ExPrep) are costly and lack cohesion into the professor\u2019s specific administrative workflow. Because of these reasons, the development of an in-house Excel course autograder was/is necessary and well-utilized.\n\nSTAKEHOLDERS\n\nThe development of an autograding software targeting such a specific niche in grading automation has many parties who rely upon it. The specific parties comprise graduate students who will have their work graded partially by the grader, the professor, who greenlights the administrative processes that are endeavored to be automated, and most directly, teaching assistants who will be using the automated results. From these parties and their various goals/expected outcomes, we derive important tenets of our project, or guiding principles that we must abide by:\n\nIntegrity: Maintaining fair and non-error grading are essential for student outcomes, who expect their deserved grades.\nEfficiency: Ensuring that the automation is effective and streamlined are necessary for the professor\u2019s goals of achieving increased grading efficiency.\nSimplicity: Prioritizing ease of use (activation and reading produced data) for TAs and other parties.\nTransparency: Ensuring that TAs and users can make clear determinations about how the autograder came to a particular score. \nReproducibility & Iteration: Unsparing use of repositories, version control, code comments, and documentation for those who may alter the program (professor, future head TAs).\n\nAnd so, the development of the grader went underway while following each of these five guiding principles. By deriving specific and necessary end goals from each related party, we are able to optimize for a product that can help meet the goals of each involved stakeholder. It is also worth it to reestablish now that the program is designed with regard to the larger grading system. That means that the auto grader will not be acting alone. It is to be integrated into the larger administrative strategy which includes a regular grading period, TA rotation, and grade release period. It is outlined below:\n\n1. Grading assigned to TAs \u2192 2. TAs grade their assigned students \u2192 3. Grades released at the end of the grading period \u2192 4. Repeat\n\nThe auto grader is to be utilized in stage (2), where the TAs grade their assigned students.\n\nASSIGNMENT OVERVIEW\n\n\tLet us make clear the structure of each assignment. Each assignment is composed of many tasks, each of which ask the student to complete a particular task like filling in cells, composing Excel formulas, and other basic or complex challenges. Each task will have multiple steps. It is at this \u201cstep\u201d level that the grader will assign points. The assignment rubrics attribute a certain amount of points for each step, and the grader awards points for the student\u2019s closeness. See the rubric below:\nFig. 1\n\nPRODUCT OVERVIEW \u2014 DESIGN METHODOLOGY\n\nThe \u2018autograder\u2019 is quite a vague term. We will elaborate further upon the \u201cnon-negotiables\u201d: i.e. the instructor\u2019s (Salman Azhar, Ph.D.) program specifications, what it is this product does, what it attempts to achieve, and how it is integrated into existing administrative and grading workflows. To give more detail, the product developed is a program which outputs grades. According to instructor (Salman Azhar, Ph.D.) specifications, two foundational design specifications are set in stone. The program must:\n\n\u2026take in a single directory file.\n\u2026export a collection of grades.\n\nAccording to these specifications, the program developed has a single input and output. Input: single directory which contains the Excel assignments for a particular assignment, and Output: a spreadsheet (.xlsm file) which contains the grade breakdown of each student for that particular assignment, as well as \u201cdebug\u201d content, which contains the grading logic used by the program. Adhering to the project specifications, all that is necessary is a Python script that is able to search a Desktop for a particular submission file, grab all of the files from within it\u2014and if they\u2019re valid assignments\u2014grade each and generate an .xlsm file to export to. To accommodate multiple unique assignments, the grader is composed of multiple files, each for a particular grader. Because of differences in computer architectures, there are two executable files for each assignment. One is a unix executable (for running on Mac) and the other is a .exe file (for running on Windows). Within the program itself, many considerations needed to be made. To adhere to our strategy for Reproducibility and Iteration (5), which aims to demystify (not that this program is in any way mystic) the program for any engineer who may improve or maintain the program, we use a fundamental programming concept known as abstraction. In this, we create multiple \u2018modules\u2019 of code to designate a particular programmatic evaluation task to a file. Each file bears the code that programmatically evaluates a particular task. In a \u2018main\u2019 file, each module is called on Excel submissions to grade it for all tasks. At a high level, here is the workflow:\n\n\nFig. 2\n\nWhere:\nMAIN holds function calls to Task A, B, C, D, and any number of individual tasks for that particular assignment. It generates a loop that evaluates each Excel assignment in INPUT DIRECTORY. It parses and stores the name of each student, computes their scores, and passes that data to the OUTPUT SHEET. Any errors/messages returned from TASK modules are relayed to the DEBUG LOG REPORT.\nTASK functionality is abstracted away from the logic of MAIN. Specific input values are fed to each module, the appropriate evaluations are computed, and a numerical score is returned from the task. If there are any computational errors or a task cannot be programmatically evaluated, the module will return an error message back to MAIN.\nINPUT DIRECTORY holds the student Excel workbooks. MAIN searches the desktop for the correct file, and pulls them.\nOUTPUT SHEET is where all grades are exported to. \nDEBUG LOG REPORT is a folder containing all of the grading comments for each student.\n \nPRODUCT OVERVIEW \u2014 GRADING METHODOLOGY\n\nAs mentioned before, there are many benefits of utilizing an algorithm in administrative and grading tasks. The lack of human cognitive time/effort overhead and time savings are both large benefits. However, as we are dealing with students, we must place a focus on our design tenets, which include Integrity (1), ensuring that the grades the TA team produces are reliable, accurate, and true to the effort that each student puts forth, as well as accurate to the assignment rubrics. This is relevant notably because the program cannot know or appreciate the spirit of the rubric or assignment\u2014it can merely perform arithmetic on a set of values passed to it. And so, it is extremely important that we are able to correctly interpret the spirit of the rubric as closely as possible and transform those heuristics into code. We\u2019ll discuss here the ideas that intertwine the grading logic of the auto grader with course grading protocols, which include A) comparison logic and B) grading fallbacks (what to do when the auto grader cannot determine a grade).\n\n\tA) COMPARISONS: As an important guardrail ensuring Integrity (1), the instructor for this project set a guideline. That is, evaluations must be made by means of comparison. What this entails is that by strictly comparing a student submission against a master key, we shrink the probability of our grader marking incorrectly at the cost of missing any other submissions which are technically correct, but do not align 100% with the rubric. What does this mean? Take an example: say the rubric for Task 3 of Assignment 1 dictates that the value \u201c3\u201d must be contained within a particular table. The exact location of that table is not specified in the rubric. Say that the master key arbitrarily has that table set starting at cell G6, but a student submission has that table that starts at cell G7. The student\u2019s content is technically correct, but by means of strict comparison with the master key, it is marked as wrong since the content is not in the same cells as the master key.\n\nThe auto grader generally utilizes this comparison approach. However, to solve the issue of a \u201cfence-post\u201d or \u201cone-off\u201d error explained earlier, Task modules are written to check for values in the specified cell of the master key, as well as the surrounding area (affectionately and aptly named: \u201cwiggle room\u201d). That way, students that may have tables \u201coff by one\u201d are marked as correct. There are a few exceptions, though:\n\nIn the case that the rubric specifies that a value must be in a particular cell, \u201cwiggle room\u201d is not put into effect.\nFor tasks involving multi-cell structures such as tables or ranges, the autograder must evaluate these cells collectively rather than individually. The system should allow the entire block to be \u201cwiggled,\u201d or shifted as a unit, to account for minor positional deviations in the student\u2019s spreadsheet. Checking cells independently creates the risk of misattributing correctness, since an identical value may coincidentally appear in an adjacent cell that does not belong to the intended structure. Such cell-level matching would produce logically incorrect grading outcomes. To avoid these false positives, the validator must confirm that the correct configuration of values appears together in the correct relative order, even if the block is offset from the expected origin.\n\nB) GRADING FALLBACKS: To remain in line with our tenet of Integrity (1), any task which cannot be reliably, scalably, or at all programmatically evaluated will fall back on manual grading, and TAs are required to check over any 0s that the auto grader awards. Why? Particular Python modules (to be discussed later) such as Openpyxl are built to evaluate Excel cells for particular values, formulas, and other attributes. However, certain issues arise when particular assignments call for checking visual forms of content like graphs. The auto grader is limited in this regard by a particular architecture (Mac x86, ARM); and due to this, some tasks may not be evaluated. If so, this will be notated in the grading output sheet.\n\nPRODUCT OVERVIEW \u2014 PROGRAM OUTPUT\n\nAs previously discussed, the program output exists in the format of two files: an .xlsm file which contains the grades of each student, as well as a directory folder which contains the debug content of each individual student in the grading session. There is much to be said about the content layout of the file\u2014adhering to our design principle of Simplicity (3), we provide organizational quirks such as: a color code for certain cell values, step-based task breakdown, score totals, and percentage of passed students for a particular task. These features beyond simply the score total exist to aid TAs in filling out each student\u2019s score sheet, which contain a field for the TA to enter. Instead, TAs can utilize the auto grader to fill in a portion or all of those fields. Additionally, the debug output can shed some additional light on what caused the student to lose a point. Since all of the programmatic, cell-checking processes are abstracted away from the TA, the debug logs exist to help them understand why a 0 might be so. The debug logs also serve as a way for TAs to \u201ccheck\u201d the auto grader\u2019s \u201cwork\u201d, abiding by our principle of Transparency (4).\n\nFig. 3\n\nTECH STACK & VERSION CONTROL\n\n\tThe autograder was developed using Python as the primary programming language due to its extensive ecosystem of data-processing libraries, strong community support, and suitability for automation tasks. The core functionality of the system relies on one major component of the Python ecosystem: Openpyxl, which makes up the full extent of Excel ingestion, validation, and output formatting. The main dependency of the project is Openpyxl, a Python library designed for reading, parsing, and manipulating Excel workbooks in the Office Open XML (.xslx) format. Openpyxl provides low-level access to spreadsheet structures, including cell objects, workbook and worksheet relationships, chart objects, styles, fills, borders, and color. Openpyxl\u2019s function allows the program to dive into Excel\u2019s underlying XML schema, making it possible to traverse cell-level metadata and access the necessary student data\u2014which are essential to all of the grading tasks that the auto grader supports.\n\nThe project was developed under a Git-based version control workflow, hosted through a private repository to ensure academic integrity. Version control played a vital role in maintaining the stability of the autograder across multiple iterations and assignments, although there was an issue with the loss of one grading module due to a computer wipe and lack of version control for that particular grading module. I learned a strong lesson, that it is important to hit \u2018git commit\u2019 after every work session. In this, the grader adheres to Reproducibility and Iteration (5), which makes it a goal to utilize version control to make technical maintenance of the program easier.\n\nUTILIZATION & FEEDBACK\n\n\tAn email request for feedback was sent to TAs who used the program following the last weeks of the semester. TAs were asked a collection of brief questions:\n\nHow was your experience using the auto-graded results?\nDid the auto-graded results save (compared to grading assignments without it) a 1) significant amount of time, 2) non-significant amount of time, 3) no time at all, or 4) did it take longer than just grading normally?\nRate the time lost/saved on a scale from 0 to 10, where 0 is \"My work took exceptionally longer using the grader\", 10 is \"My time was tremendously saved\", and 5 is \"no time lost or saved\".\nHow easy or difficult was it to find specific names/grades for students on the spreadsheet? Did color-coding help or hurt your progress?\nDid you encounter any technical or scoring issues with the auto-grader? If so, please explain briefly. If not, write \"N/a\".\nIn a few words, describe how you feel about the auto grader. What would you improve?\n\nUser feedback is extremely important for a program. By receiving user feedback, one is able to understand the pain points of their application, revise, iterate, and make the product better than what it was. By asking these specific questions, I sought to understand the effectiveness and utilization of our design tenets. Specifically, did the grader increase efficiency, prioritize ease of use for TAs, and ensure that TAs and users can make clear determinations about how the autograder came to a particular score (Efficiency (2), Simplicity (3), Transparency (4))? Below are my findings on helpfulness, including A) Navigation Efficiency, B) Visual Design, and C) Technical Challenges.\n\nTheir responses gave a mixture of positive sentiment, early-stage adjustment challenges, and clear indications of where Version 2 of the autograder should focus improvement efforts. Most TAs reported that the autograder was generally useful during the grading process. Several noted that the tool became easier to use over time as they became more familiar with its structure. One TA described the experience as \u201cgood and helpful.\u201d Another noted that it was \u201cfair\u201d but became better after they gained familiarity. One initially found it difficult and chose not to use it at first, but later acknowledged the tool\u2019s value after revisiting it. A recurring theme was that there was an initial learning curve where learning how to efficiently navigate a new spreadsheet was difficult, especially during a busy academic term.\n\nGRADING EFFICIENCY: Feedback on efficiency varied, as some TAs reported that the autograder saved a significant amount of time. Others reported non-significant time savings, largely because they double-checked the autograder's results during early use. One TA found usage easier over time but noted that initial unfamiliarity reduced time savings. Only one TA rated time savings below the average (4/10), primarily due to workflow friction (scrolling between graded vs. ungraded portions). Reported time-savings scores ranged from 4 to ~7, with most around 6. No TA reported that the tool increased time burden overall. A few TAs noted that distrust of automation early in the semester led them to manually re-check student work, which reduced efficiency until familiarity increased.\n\nVISUAL DESIGN: Most TAs found color-coding helpful. It supported organization, reduced visual clutter, and helped them quickly locate the parts they were responsible for. They also found name lookup moderately easy, though one TA noted difficulty because names in the autograder results did not match the Canvas ordering, requiring manual cross-referencing. Some suggested separating first and last names for improved readability. Overall, the visual design and formatting were seen as strengths.\n\nTECHNICAL CHALLENGES: Some TAs encountered minor technical or scoring problems. Common issues were False negatives; like when students placed valid answers in a different but reasonable cell, the autograder marked the answer wrong. To note, this was an intentional design, as it was a required aspect of maintaining grading Integrity (1). For any 0s, TAs should manually check. However, no TAs reported outright incorrect scoring that misled their final grading decisions.\n\nTAs generally expressed positive or cautiously positive impressions. Some words used were \u201cHelpful\u201d, \u201cGood\u201d, \u201cPretty helpful and easy on the eye\u201d, as it became more useful over time for many TAs. In terms of achieving Efficiency (2), Most TAs reported meaningful though sometimes limited time savings, with scores averaging around 6/10. Efficiency improved as familiarity increased. The largest efficiency losses came from workflow friction (Canvas ordering mismatch) and partial grading coverage. For Simplicity (3), color-coding and the visual layout supported ease of use. Difficulties stemmed primarily from name-order mismatches and the mental overhead of learning a new tool mid-semester. For Transparency (4), while most TAs trusted the tool, lack of information on cell-specific matching and uncertainty about what was or wasn\u2019t graded reduced transparency. TAs want clearer delineation of grading boundaries and more flexible evaluation logic.\nCONCLUSION & NEXT STEPS\n\nThe feedback collected throughout the semester makes clear that the autograder successfully addressed several core administrative challenges in DECISION 616, while also revealing important areas for refinement. The tool seemingly  improved grading output, standardized evaluation, and provided a structured, reproducible method for scoring complex Excel-based assignments. However, as with any software deployed in a real instructional environment, Version 1 faced many limitations which could be fixed in a second version. In direct response to TA feedback\u2014and guided by the project\u2019s principles of Efficiency, Simplicity, Transparency, Integrity, and Reproducibility\u2014the next version would benefit from expanded grading coverage, more flexible comparison logic, improved name matching and navigation, and streamlined debugging output.\n\nBecause of reduced TA headcount and high enrollment in particular courses\u2014such a tool fills an important gap in automated grading and administration. With the project\u2019s foundational design tenets derived from analyzing stakeholder goals, it respects the integrity of student work, supports graders, and maintains the high bar of grading for students. The autograder has already delivered measurable benefits to the DECISION 616 team. TAs reported moderate to significant time savings, clearer organization, and improved consistency across the grading process. At the same time, their feedback revealed opportunities to strengthen the tool\u2019s accuracy, usability, and transparency. In the end, this project also goes back to the broader goal of software development: creating tools that not only work, but work well for people. With continued refinement, the program has the potential to become an invaluable asset not only for DECISION 616, but for a broader range of courses that rely on spreadsheet-based analytics.\n    \n    GRADING CONTEXT:\n    RUBRIC:\n{\n  \"sheets\": {\n    \"Sheet1\": {\n      \"cells\": {\n        \"A1\": {\n          \"value\": \"Criterion\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"B1\": {\n          \"value\": \"Excellent (90-100%)\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"C1\": {\n          \"value\": \"Satisfactory (75-89%)\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"D1\": {\n          \"value\": \"Needs Improvement (<75%)\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"E1\": {\n          \"value\": \"Score\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"A2\": {\n          \"value\": \"Problem Definition & Exigence\",\n          \"formula\": null,\n          \"style_idx\": \"2\"\n        },\n        \"B2\": {\n          \"value\": \"Clearly articulates the \\\"why\\\" behind the project (budget cuts, enrollment vs. TA ratio). Connects CS principles to specific administrative needs.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"C2\": {\n          \"value\": \"Identifies the problem and the need for automation but lacks deep connection to the specific course context.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"D2\": {\n          \"value\": \"Vague description of the problem; fails to explain why a new solution was needed over existing tools.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"E2\": {\n          \"value\": \"/20\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"A3\": {\n          \"value\": \"Technical Design & Methodology\",\n          \"formula\": null,\n          \"style_idx\": \"2\"\n        },\n        \"B3\": {\n          \"value\": \"Thoroughly explains the \\\"Main vs. Task\\\" abstraction. Demonstrates a deep understanding of the .xlsx XML structure and Openpyxl logic.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"C3\": {\n          \"value\": \"Explains the basic flow of the program. Mentions the tech stack (Python/Openpyxl) without much detail on implementation.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"D3\": {\n          \"value\": \"Minimal technical detail; does not explain how the code actually interacts with the student files.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"E3\": {\n          \"value\": \"/25\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"A4\": {\n          \"value\": \"Design Principles (The 5 Tenets)\",\n          \"formula\": null,\n          \"style_idx\": \"2\"\n        },\n        \"B4\": {\n          \"value\": \"Successfully integrates Integrity, Efficiency, Simplicity, Transparency, and Reproducibility throughout the entire narrative.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"C4\": {\n          \"value\": \"Mentions the tenets but does not consistently apply them to the product features described.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"D4\": {\n          \"value\": \"Fails to mention or poorly defines the guiding principles of the project.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"E4\": {\n          \"value\": \"/15\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"A5\": {\n          \"value\": \"Critical Reflection & Evaluation\",\n          \"formula\": null,\n          \"style_idx\": \"2\"\n        },\n        \"B5\": {\n          \"value\": \"Candidly discusses technical debt (loss of code), \\\"fence-post\\\" errors, and analyzes TA feedback data to suggest specific V2 improvements.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"C5\": {\n          \"value\": \"Includes TA feedback but provides limited analysis on how that feedback will change the technical roadmap.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"D5\": {\n          \"value\": \"No reflection on failures or user feedback; purely descriptive rather than analytical.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"E5\": {\n          \"value\": \"/20\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"A6\": {\n          \"value\": \"Organization & Professionalism\",\n          \"formula\": null,\n          \"style_idx\": \"2\"\n        },\n        \"B6\": {\n          \"value\": \"Logical flow with clear headings. Use of \\\"Figures\\\" and professional tone suitable for an Independent Study report.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"C6\": {\n          \"value\": \"Mostly organized, but transitions between the technical stack and the TA feedback sections feel abrupt.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"D6\": {\n          \"value\": \"Disorganized; lacks clear structure or professional academic tone.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"E6\": {\n          \"value\": \"/10\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"A7\": {\n          \"value\": \"Format & Documentation\",\n          \"formula\": null,\n          \"style_idx\": \"2\"\n        },\n        \"B7\": {\n          \"value\": \"Correct use of citations (if any) and clear labeling of the project stakeholders and metadata.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"C7\": {\n          \"value\": \"Minor formatting issues or inconsistent labeling of figures/sections.\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"D7\": {\n          \"value\": \"Lacks basic report metadata (date, advisor names, title).\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        },\n        \"E7\": {\n          \"value\": \"/10\",\n          \"formula\": null,\n          \"style_idx\": \"1\"\n        }\n      },\n      \"metadata\": {\n        \"validations\": [],\n        \"conditional_formatting\": [],\n        \"drawings\": [\n          \"Drawing Reference rId=rId1\"\n        ]\n      }\n    }\n  }\n}\n    \n    output ONLY valid JSON.\n    "
}